{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "from IPython.display import clear_output\n",
    "import pickle\n",
    "import os\n",
    "# import search\n",
    "import math\n",
    "from collections import deque\n",
    "from typing import Deque, Dict, List, Tuple\n",
    "import gymnasium as gym\n",
    "from time import time\n",
    "# import moviepy \n",
    "\n",
    "# from segment_tree import MinSegmentTree, SumSegmentTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Segment tree for Prioritized Replay Buffer.\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "class SegmentTree:\n",
    "    \"\"\" Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    Attributes:\n",
    "        capacity (int)\n",
    "        tree (list)\n",
    "        operation (function)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int, operation: Callable, init_value: float):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "            operation (function)\n",
    "            init_value (float)\n",
    "\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            capacity > 0 and capacity & (capacity - 1) == 0\n",
    "        ), \"capacity must be positive and a power of 2.\"\n",
    "        self.capacity = capacity\n",
    "        self.tree = [init_value for _ in range(2 * capacity)]\n",
    "        self.operation = operation\n",
    "\n",
    "    def _operate_helper(\n",
    "        self, start: int, end: int, node: int, node_start: int, node_end: int\n",
    "    ) -> float:\n",
    "        \"\"\"Returns result of operation in segment.\"\"\"\n",
    "        if start == node_start and end == node_end:\n",
    "            return self.tree[node]\n",
    "        mid = (node_start + node_end) // 2\n",
    "        if end <= mid:\n",
    "            return self._operate_helper(start, end, 2 * node, node_start, mid)\n",
    "        else:\n",
    "            if mid + 1 <= start:\n",
    "                return self._operate_helper(start, end, 2 * node + 1, mid + 1, node_end)\n",
    "            else:\n",
    "                return self.operation(\n",
    "                    self._operate_helper(start, mid, 2 * node, node_start, mid),\n",
    "                    self._operate_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end),\n",
    "                )\n",
    "\n",
    "    def operate(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns result of applying `self.operation`.\"\"\"\n",
    "        if end <= 0:\n",
    "            end += self.capacity\n",
    "        end -= 1\n",
    "\n",
    "        return self._operate_helper(start, end, 1, 0, self.capacity - 1)\n",
    "\n",
    "    def __setitem__(self, idx: int, val: float):\n",
    "        \"\"\"Set value in tree.\"\"\"\n",
    "        idx += self.capacity\n",
    "        self.tree[idx] = val\n",
    "\n",
    "        idx //= 2\n",
    "        while idx >= 1:\n",
    "            self.tree[idx] = self.operation(self.tree[2 * idx], self.tree[2 * idx + 1])\n",
    "            idx //= 2\n",
    "\n",
    "    def __getitem__(self, idx: int) -> float:\n",
    "        \"\"\"Get real value in leaf node of tree.\"\"\"\n",
    "        assert 0 <= idx < self.capacity\n",
    "\n",
    "        return self.tree[self.capacity + idx]\n",
    "\n",
    "\n",
    "class SumSegmentTree(SegmentTree):\n",
    "    \"\"\" Create SumSegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(SumSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=operator.add, init_value=0.0\n",
    "        )\n",
    "\n",
    "    def sum(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns arr[start] + ... + arr[end].\"\"\"\n",
    "        return super(SumSegmentTree, self).operate(start, end)\n",
    "\n",
    "    def retrieve(self, upperbound: float) -> int:\n",
    "        \"\"\"Find the highest index `i` about upper bound in the tree\"\"\"\n",
    "        # TODO: Check assert case and fix bug\n",
    "        assert 0 <= upperbound <= self.sum() + 1e-5, \"upperbound: {}\".format(upperbound)\n",
    "\n",
    "        idx = 1\n",
    "\n",
    "        while idx < self.capacity:  # while non-leaf\n",
    "            left = 2 * idx\n",
    "            right = left + 1\n",
    "            if self.tree[left] > upperbound:\n",
    "                idx = 2 * idx\n",
    "            else:\n",
    "                upperbound -= self.tree[left]\n",
    "                idx = right\n",
    "        return idx - self.capacity\n",
    "\n",
    "\n",
    "class MinSegmentTree(SegmentTree):\n",
    "    \"\"\" Create SegmentTree.\n",
    "\n",
    "    Taken from OpenAI baselines github repository:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/common/segment_tree.py\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int):\n",
    "        \"\"\"Initialization.\n",
    "\n",
    "        Args:\n",
    "            capacity (int)\n",
    "\n",
    "        \"\"\"\n",
    "        super(MinSegmentTree, self).__init__(\n",
    "            capacity=capacity, operation=min, init_value=float(\"inf\")\n",
    "        )\n",
    "\n",
    "    def min(self, start: int = 0, end: int = 0) -> float:\n",
    "        \"\"\"Returns min(arr[start], ...,  arr[end]).\"\"\"\n",
    "        return super(MinSegmentTree, self).operate(start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, observation_dimensions, max_size: int, batch_size = 32, n_step = 1, gamma = 0.99):\n",
    "        # self.observation_buffer = np.zeros((max_size,) + observation_dimensions, dtype=np.float32)\n",
    "        # self.next_observation_buffer = np.zeros((max_size,) + observation_dimensions, dtype=np.float32)\n",
    "        observation_buffer_shape = []\n",
    "        observation_buffer_shape += [max_size]\n",
    "        observation_buffer_shape += list(observation_dimensions)\n",
    "        observation_buffer_shape = list(observation_buffer_shape)\n",
    "        self.observation_buffer = np.zeros(observation_buffer_shape, dtype=np.float32)\n",
    "        self.next_observation_buffer = np.zeros(observation_buffer_shape, dtype=np.float32)\n",
    "        self.action_buffer = np.zeros(max_size, dtype=np.int32)\n",
    "        self.reward_buffer = np.zeros(max_size, dtype=np.float32)\n",
    "        self.done_buffer = np.zeros(max_size)\n",
    "\n",
    "        self.max_size = max_size\n",
    "        self.batch_size = batch_size\n",
    "        self.pointer = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # n-step learning\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def store(self, observation, action, reward, next_observation, done):\n",
    "        print(\"Storing in Buffer\")\n",
    "        time1 = 0\n",
    "        time1 = time()\n",
    "        transition = (observation, action, reward, next_observation, done)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            print(\"Buffer Storage Time \", time() - time1)\n",
    "            return ()\n",
    "        \n",
    "        # compute n-step return and store\n",
    "        reward, next_observation, done = self._get_n_step_info()\n",
    "        observation, action = self.n_step_buffer[0][:2]\n",
    "        self.observation_buffer[self.pointer] = observation\n",
    "        self.action_buffer[self.pointer] = action\n",
    "        self.reward_buffer[self.pointer] = reward\n",
    "        self.next_observation_buffer[self.pointer] = next_observation\n",
    "        self.done_buffer[self.pointer] = done\n",
    "\n",
    "        self.pointer = (self.pointer + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "        \n",
    "        print(\"Buffer Storage Time \", time1- time())\n",
    "        return self.n_step_buffer[0]\n",
    "    \n",
    "    def sample(self):\n",
    "        print(\"Sampling From Buffer\")\n",
    "        time1 = time()\n",
    "        idx = np.random.choice(self.size, self.batch_size, replace=False)\n",
    "\n",
    "        print(\"Buffer Sampling Time \", time() - time1)\n",
    "        return dict(\n",
    "            observations=self.observation_buffer[idx],\n",
    "            next_observations=self.next_observation_buffer[idx],\n",
    "            actions=self.action_buffer[idx],\n",
    "            rewards=self.reward_buffer[idx],\n",
    "            dones=self.done_buffer[idx],\n",
    "        )\n",
    "\n",
    "    def sample_from_indices(self, indices):\n",
    "        print(\"Sampling From Indices\")\n",
    "        return dict(\n",
    "            observations=self.observation_buffer[indices],\n",
    "            next_observations=self.next_observation_buffer[indices],\n",
    "            actions=self.action_buffer[indices],\n",
    "            rewards=self.reward_buffer[indices],\n",
    "            dones=self.done_buffer[indices],\n",
    "        )\n",
    "\n",
    "    def _get_n_step_info(self):\n",
    "        reward, next_observation, done = self.n_step_buffer[-1][-3:]\n",
    "\n",
    "        for transition in reversed(list(self.n_step_buffer)[:-1]):\n",
    "            r, n_o, d = transition[-3:]\n",
    "            reward = r + self.gamma * reward * (1 - d)\n",
    "            next_observation, done = (n_o, d) if d else (next_observation, done)\n",
    "\n",
    "        return reward, next_observation, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer(ReplayBuffer):\n",
    "    def __init__(\n",
    "            self,\n",
    "            observation_dimensions,\n",
    "            max_size,\n",
    "            batch_size=32,\n",
    "            max_priority=1.0,\n",
    "            alpha=0.6,\n",
    "            n_step=1,\n",
    "            gamma=0.99,\n",
    "        ):\n",
    "        assert alpha >= 0\n",
    "\n",
    "        super(PrioritizedReplayBuffer, self).__init__(\n",
    "            observation_dimensions, max_size, batch_size, n_step=n_step, gamma=gamma\n",
    "        )\n",
    "\n",
    "        self.max_priority = max_priority  # (initial) priority\n",
    "        self.tree_pointer = 0\n",
    "\n",
    "        self.alpha = alpha  # Hyperparameter that we use to make a tradeoff between taking only exp with high priority and sampling randomly\n",
    "\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "            \n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "    \n",
    "    def store(self, observation, action, reward, next_observation, done):\n",
    "        print(\"Storing in PrioritizedReplayBuffer\")\n",
    "        time1 = 0\n",
    "        time1 = time()\n",
    "        transition = super().store(observation, action, reward, next_observation, done)\n",
    "\n",
    "        if transition:\n",
    "            self.sum_tree[self.tree_pointer] = self.max_priority ** self.alpha\n",
    "            self.min_tree[self.tree_pointer] = self.max_priority ** self.alpha\n",
    "            self.tree_pointer = (self.tree_pointer + 1) % self.max_size\n",
    "\n",
    "        print(\"Storing in PrioritizedReplayBuffer Time \", time() - time1)\n",
    "        return transition\n",
    "    \n",
    "    def sample(self, beta=0.4):\n",
    "        print(\"Sampling from PrioritizedReplayBuffer\")\n",
    "        time1 = 0\n",
    "        time1 = time()\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "\n",
    "        indices = self._sample_proportional()        \n",
    "        print(\"Retrieving Data from PrioritizedReplayBuffer Data Arrays\")\n",
    "        time2 = 0\n",
    "        time2 = time()\n",
    "        observations = self.observation_buffer[indices]\n",
    "        next_observations = self.next_observation_buffer[indices]\n",
    "        actions = self.action_buffer[indices]\n",
    "        rewards = self.reward_buffer[indices]\n",
    "        dones = self.done_buffer[indices]\n",
    "        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n",
    "        print(\"Retrieving Data from PrioritizedReplayBuffer Data Arrays Time \", time() - time2)\n",
    "\n",
    "        print(\"Sampling from PrioritizedReplayBuffer Time \", time() - time1)\n",
    "        return dict(\n",
    "            observations=observations,\n",
    "            next_observations=next_observations,\n",
    "            actions=actions,\n",
    "            rewards=rewards,\n",
    "            dones=dones,\n",
    "            weights=weights,\n",
    "            indices=indices,\n",
    "        )\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for index, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= index < len(self)\n",
    "\n",
    "            self.sum_tree[index] = priority ** self.alpha\n",
    "            self.min_tree[index] = priority ** self.alpha\n",
    "            self.max_priority = max(self.max_priority, priority) # could remove and clip priorities in experience replay isntead\n",
    "    \n",
    "    def _sample_proportional(self):\n",
    "        print(\"Getting Indices from PrioritizedReplayBuffer Sum Tree\")\n",
    "        time1 = 0\n",
    "        time1 = time()\n",
    "        indices = []\n",
    "        total_priority = self.sum_tree.sum(0, len(self) - 1)\n",
    "        priority_segment = total_priority / self.batch_size\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            a = priority_segment * i\n",
    "            b = priority_segment * (i + 1)\n",
    "            upperbound = np.random.uniform(a, b)\n",
    "            index = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(index)\n",
    "        \n",
    "        print(\"Getting Indices from PrioritizedReplayBuffer Sum Tree Time \", time() - time1)\n",
    "        return indices\n",
    "\n",
    "    def _calculate_weight(self, index, beta):\n",
    "        min_priority = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (min_priority * len(self)) ** (-beta)\n",
    "        priority_sample = self.sum_tree[index] / self.sum_tree.sum()\n",
    "        weight = (priority_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "\n",
    "        return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From tensorflow_addons\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import (\n",
    "    activations,\n",
    "    initializers,\n",
    "    regularizers,\n",
    "    constraints,\n",
    ")\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import InputSpec\n",
    "\n",
    "def _scaled_noise(size, dtype):\n",
    "    x = tf.random.normal(shape=size, dtype=dtype)\n",
    "    return tf.sign(x) * tf.sqrt(tf.abs(x))\n",
    "\n",
    "class NoisyDense(tf.keras.layers.Dense):\n",
    "    def __init__(\n",
    "        self,\n",
    "        units: int,\n",
    "        sigma: float = 0.5, # might want to make sigma 0.1 for CPU's \n",
    "        use_factorised: bool = True,\n",
    "        activation = None,\n",
    "        use_bias: bool = True,\n",
    "        kernel_regularizer = None,\n",
    "        bias_regularizer = None,\n",
    "        activity_regularizer = None,\n",
    "        kernel_constraint = None,\n",
    "        bias_constraint = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            units=units,\n",
    "            activation=activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            **kwargs,\n",
    "        )\n",
    "        delattr(self, \"kernel_initializer\")\n",
    "        delattr(self, \"bias_initializer\")\n",
    "        self.sigma = sigma\n",
    "        self.use_factorised = use_factorised\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Make sure dtype is correct\n",
    "        dtype = tf.dtypes.as_dtype(self.dtype or K.floatx())\n",
    "        if not (dtype.is_floating or dtype.is_complex):\n",
    "            raise TypeError(\n",
    "                \"Unable to build `Dense` layer with non-floating point \"\n",
    "                \"dtype %s\" % (dtype,)\n",
    "            )\n",
    "\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        self.last_dim = tf.compat.dimension_value(input_shape[-1])\n",
    "        sqrt_dim = self.last_dim ** (1 / 2)\n",
    "        if self.last_dim is None:\n",
    "            raise ValueError(\n",
    "                \"The last dimension of the inputs to `Dense` \"\n",
    "                \"should be defined. Found `None`.\"\n",
    "            )\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: self.last_dim})\n",
    "\n",
    "        # use factorising Gaussian variables\n",
    "        if self.use_factorised:\n",
    "            mu_init = 1.0 / sqrt_dim\n",
    "            sigma_init = self.sigma / sqrt_dim\n",
    "        # use independent Gaussian variables\n",
    "        else:\n",
    "            mu_init = (3.0 / self.last_dim) ** (1 / 2)\n",
    "            sigma_init = 0.017\n",
    "\n",
    "        sigma_init = initializers.Constant(value=sigma_init)\n",
    "        mu_init = initializers.RandomUniform(minval=-mu_init, maxval=mu_init)\n",
    "\n",
    "        # Learnable parameters\n",
    "        self.sigma_kernel = self.add_weight(\n",
    "            \"sigma_kernel\",\n",
    "            shape=[self.last_dim, self.units],\n",
    "            initializer=sigma_init,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.mu_kernel = self.add_weight(\n",
    "            \"mu_kernel\",\n",
    "            shape=[self.last_dim, self.units],\n",
    "            initializer=mu_init,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "            dtype=self.dtype,\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        self.eps_kernel = self.add_weight(\n",
    "            \"eps_kernel\",\n",
    "            shape=[self.last_dim, self.units],\n",
    "            initializer=initializers.Zeros(),\n",
    "            regularizer=None,\n",
    "            constraint=None,\n",
    "            dtype=self.dtype,\n",
    "            trainable=False,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            self.sigma_bias = self.add_weight(\n",
    "                \"sigma_bias\",\n",
    "                shape=[\n",
    "                    self.units,\n",
    "                ],\n",
    "                initializer=sigma_init,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                dtype=self.dtype,\n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "            self.mu_bias = self.add_weight(\n",
    "                \"mu_bias\",\n",
    "                shape=[\n",
    "                    self.units,\n",
    "                ],\n",
    "                initializer=mu_init,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "                dtype=self.dtype,\n",
    "                trainable=True,\n",
    "            )\n",
    "\n",
    "            self.eps_bias = self.add_weight(\n",
    "                \"eps_bias\",\n",
    "                shape=[\n",
    "                    self.units,\n",
    "                ],\n",
    "                initializer=initializers.Zeros(),\n",
    "                regularizer=None,\n",
    "                constraint=None,\n",
    "                dtype=self.dtype,\n",
    "                trainable=False,\n",
    "            )\n",
    "        else:\n",
    "            self.sigma_bias = None\n",
    "            self.mu_bias = None\n",
    "            self.eps_bias = None\n",
    "        self.reset_noise()\n",
    "        self.built = True\n",
    "\n",
    "    @property\n",
    "    def kernel(self):\n",
    "        return self.mu_kernel + (self.sigma_kernel * self.eps_kernel)\n",
    "\n",
    "    @property\n",
    "    def bias(self):\n",
    "        if self.use_bias:\n",
    "            return self.mu_bias + (self.sigma_bias * self.eps_bias)\n",
    "\n",
    "    def reset_noise(self):\n",
    "        \"\"\"Create the factorised Gaussian noise.\"\"\"\n",
    "\n",
    "        if self.use_factorised:\n",
    "            # Generate random noise\n",
    "            in_eps = _scaled_noise([self.last_dim, 1], dtype=self.dtype)\n",
    "            out_eps = _scaled_noise([1, self.units], dtype=self.dtype)\n",
    "\n",
    "            # Scale the random noise\n",
    "            self.eps_kernel.assign(tf.matmul(in_eps, out_eps))\n",
    "            self.eps_bias.assign(out_eps[0])\n",
    "        else:\n",
    "            # generate independent variables\n",
    "            self.eps_kernel.assign(\n",
    "                tf.random.normal(shape=[self.last_dim, self.units], dtype=self.dtype)\n",
    "            )\n",
    "            self.eps_bias.assign(\n",
    "                tf.random.normal(\n",
    "                    shape=[\n",
    "                        self.units,\n",
    "                    ],\n",
    "                    dtype=self.dtype,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def remove_noise(self):\n",
    "        \"\"\"Remove the factorised Gaussian noise.\"\"\"\n",
    "\n",
    "        self.eps_kernel.assign(tf.zeros([self.last_dim, self.units], dtype=self.dtype))\n",
    "        self.eps_bias.assign(tf.zeros([self.units], dtype=self.dtype))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # TODO(WindQAQ): Replace this with `dense()` once public.\n",
    "        return super().call(inputs)\n",
    "\n",
    "    def get_config(self):\n",
    "        # TODO(WindQAQ): Get rid of this hacky way.\n",
    "        config = super(tf.keras.layers.Dense, self).get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"units\": self.units,\n",
    "                \"sigma\": self.sigma,\n",
    "                \"use_factorised\": self.use_factorised,\n",
    "                \"activation\": activations.serialize(self.activation),\n",
    "                \"use_bias\": self.use_bias,\n",
    "                \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\n",
    "                \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n",
    "                \"activity_regularizer\": regularizers.serialize(\n",
    "                    self.activity_regularizer\n",
    "                ),\n",
    "                \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n",
    "                \"bias_constraint\": constraints.serialize(self.bias_constraint),\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(tf.keras.Model):\n",
    "    def __init__(self, config, output_size, input_shape, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.inputs = tf.keras.layers.Input(shape=input_shape, name='my_input')\n",
    "        self.has_conv_layers = len(config['conv_layers']) > 0\n",
    "        self.has_dense_layers = len(config['dense_layers']) > 0\n",
    "        if self.has_conv_layers:\n",
    "            self.conv_layers = []        \n",
    "            for i, (filters, kernel_size, strides) in enumerate(config['conv_layers']):\n",
    "                if config['conv_layers_noisy']:\n",
    "                    # if i == 0:\n",
    "                    #     self.conv_layers.append(NoisyConv2D(filters, kernel_size, strides=strides, kernel_initializer=config['kernel_initializer'], activation=config['activation'], input_shape=input_shape))\n",
    "                    # else:\n",
    "                    #     self.conv_layers.append(NoisyConv2D(filters, kernel_size, strides=strides, kernel_initializer=config['kernel_initializer'], activation=config['activation']))\n",
    "                    pass\n",
    "                else:\n",
    "                    if i == 0:\n",
    "                        self.conv_layers.append(tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, kernel_initializer=config['kernel_initializer'], activation=config['activation'], input_shape=input_shape, padding='same'))\n",
    "                    else:\n",
    "                        self.conv_layers.append(tf.keras.layers.Conv2D(filters, kernel_size, strides=strides, kernel_initializer=config['kernel_initializer'], activation=config['activation'], padding='same'))\n",
    "            self.conv_layers.append(tf.keras.layers.Flatten())\n",
    "        \n",
    "        if self.has_dense_layers:\n",
    "            self.dense_layers = []\n",
    "            for i, units in enumerate(config['dense_layers']):\n",
    "                if config['dense_layers_noisy']:\n",
    "                    self.dense_layers.append(NoisyDense(units, sigma=config['noisy_sigma'], kernel_initializer=config['kernel_initializer'], activation=config['activation']))\n",
    "                else:\n",
    "                    self.dense_layers.append(tf.keras.layers.Dense(units, kernel_initializer=config['kernel_initializer'], activation=config['activation']))\n",
    "\n",
    "        self.has_value_hidden_layers = len(config['value_hidden_layers']) > 0\n",
    "        if self.has_value_hidden_layers:\n",
    "            self.value_hidden_layers = []\n",
    "            for i, units in enumerate(config['value_hidden_layers']):\n",
    "                self.value_hidden_layers.append(NoisyDense(units, sigma=config['noisy_sigma'], kernel_initializer=config['kernel_initializer'], activation=config['activation']))\n",
    "        \n",
    "        self.value = NoisyDense(\n",
    "            config[\"atom_size\"], sigma=config['noisy_sigma'], kernel_initializer=config['kernel_initializer'], activation=\"linear\", name=\"HiddenV\"\n",
    "        )\n",
    "        \n",
    "        self.has_advantage_hidden_layers = len(config['advantage_hidden_layers']) > 0\n",
    "        if self.has_advantage_hidden_layers:\n",
    "            self.advantage_hidden_layers = []\n",
    "            for i, units in enumerate(config['advantage_hidden_layers']):\n",
    "                self.advantage_hidden_layers.append(NoisyDense(units, sigma=config['noisy_sigma'], kernel_initializer=config['kernel_initializer'], activation=config['activation']))\n",
    "        \n",
    "        self.advantage = NoisyDense(config[\"atom_size\"] * output_size, sigma=config['noisy_sigma'], kernel_initializer=config['kernel_initializer'], activation=\"linear\", name=\"A\")\n",
    "        self.advantage_reduced_mean = tf.keras.layers.Lambda(\n",
    "            lambda a: a - tf.reduce_mean(a, axis=1, keepdims=True), name=\"Ao\"\n",
    "        )\n",
    "        \n",
    "        self.advantage_reshaped = tf.keras.layers.Reshape((output_size, config[\"atom_size\"]), name=\"ReshapeAo\")\n",
    "        self.value_reshaped = tf.keras.layers.Reshape((1, config[\"atom_size\"]), name=\"ReshapeV\")\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        # self.softmax = tf.keras.activations.softmax(self.add, axis=-1)\n",
    "        self.clip_qs = tf.keras.layers.Lambda(\n",
    "            lambda q: tf.clip_by_value(q, 1e-3, 1), name=\"ClippedQ\"\n",
    "        )\n",
    "        self.outputs = tf.keras.layers.Lambda(\n",
    "            lambda q: tf.reduce_sum(q * config['support'], axis=2), name=\"Q\"\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        x = inputs\n",
    "        if self.has_conv_layers:\n",
    "            for layer in self.conv_layers:\n",
    "                x = layer(x)\n",
    "        if self.has_dense_layers:\n",
    "            for layer in self.dense_layers:\n",
    "                x = layer(x)\n",
    "        if self.has_value_hidden_layers:\n",
    "            for layer in self.value_hidden_layers:\n",
    "                x = layer(x)\n",
    "        value = self.value(x)\n",
    "        value = self.value_reshaped(value)\n",
    "\n",
    "        if self.has_advantage_hidden_layers:\n",
    "            for layer in self.advantage_hidden_layers:\n",
    "                x = layer(x)\n",
    "        advantage = self.advantage(x)\n",
    "        advantage = self.advantage_reduced_mean(advantage)\n",
    "        advantage = self.advantage_reshaped(advantage)\n",
    "\n",
    "        q = self.add([value, advantage])\n",
    "        q = tf.keras.activations.softmax(q, axis=-1)\n",
    "        q = self.clip_qs(q)\n",
    "        # q = self.outputs(q)\n",
    "        return q\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        if self.config['conv_layers_noisy']:\n",
    "            for layer in self.conv_layers:\n",
    "                layer.reset_noise()\n",
    "        if self.config['dense_layers_noisy']:\n",
    "            for layer in self.dense_layers:\n",
    "                layer.reset_noise()\n",
    "        if self.has_value_hidden_layers:\n",
    "            for layer in self.value_hidden_layers:\n",
    "                layer.reset_noise()\n",
    "        if self.has_advantage_hidden_layers:\n",
    "            for layer in self.advantage_hidden_layers:\n",
    "                layer.reset_noise()\n",
    "        self.value.reset_noise()\n",
    "        self.advantage.reset_noise()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RainbowDQN:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        model_name=datetime.datetime.now().timestamp(),\n",
    "        config=None,\n",
    "        start_episode=0,\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.model_name = model_name\n",
    "        self.env = env\n",
    "        self.observation_dimensions = env.observation_space.shape\n",
    "        self.num_actions = env.action_space.n\n",
    "\n",
    "        self.model = Network(config, self.num_actions, input_shape=self.observation_dimensions)\n",
    "\n",
    "        self.target_model = Network(config, self.num_actions, input_shape=self.observation_dimensions)\n",
    "\n",
    "        self.optimizer = config[\"optimizer_function\"]\n",
    "        self.adam_epsilon=config[\"adam_epsilon\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.loss_function = config[\"loss_function\"]\n",
    "        self.clipnorm = 10.0\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=self.optimizer(learning_rate=self.learning_rate, epsilon=self.adam_epsilon, clipnorm=self.clipnorm),\n",
    "            loss=config[\"loss_function\"],\n",
    "        )\n",
    "\n",
    "        self.target_model.compile(\n",
    "            optimizer=self.optimizer(learning_rate=self.learning_rate, epsilon=self.adam_epsilon, clipnorm=self.clipnorm),\n",
    "            loss=config[\"loss_function\"],\n",
    "        )\n",
    "\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "        \n",
    "        self.num_training_steps = int(config[\"num_training_steps\"])\n",
    "        self.start_episode = start_episode\n",
    "\n",
    "        self.discount_factor = config[\"discount_factor\"]\n",
    "        \n",
    "        self.replay_batch_size = int(config[\"replay_batch_size\"])\n",
    "        self.replay_period = int(config[\"replay_period\"])\n",
    "        self.memory_size = max(int(config[\"memory_size\"]), self.replay_batch_size)\n",
    "        self.min_memory_size = int(config[\"min_memory_size\"])\n",
    "\n",
    "        self.soft_update = config[\"soft_update\"]\n",
    "        self.transfer_frequency = int(config[\"transfer_frequency\"])\n",
    "        self.ema_beta = config[\"ema_beta\"]\n",
    "\n",
    "        self.per_epsilon = config[\"per_epsilon\"]\n",
    "        self.per_beta = config[\"per_beta\"]\n",
    "        self.per_beta_increase = config[\"per_beta_increase\"]\n",
    "        self.memory = PrioritizedReplayBuffer(\n",
    "            observation_dimensions=self.observation_dimensions,\n",
    "            max_size=self.memory_size,\n",
    "            batch_size=self.replay_batch_size,\n",
    "            max_priority=1.0,\n",
    "            alpha=config[\"per_alpha\"],\n",
    "            n_step=config[\"n_step\"],\n",
    "            gamma=config[\"discount_factor\"],\n",
    "        )\n",
    "\n",
    "        self.use_n_step = config[\"n_step\"] > 1\n",
    "            \n",
    "        self.n_step = config[\"n_step\"]\n",
    "        \n",
    "        if self.use_n_step:\n",
    "            self.memory_n = ReplayBuffer(\n",
    "                observation_dimensions=self.observation_dimensions,\n",
    "                max_size=self.memory_size,\n",
    "                batch_size=self.replay_batch_size,\n",
    "                n_step=self.n_step,\n",
    "                gamma=config[\"discount_factor\"],\n",
    "            )\n",
    "\n",
    "        self.v_min = config[\"v_min\"]\n",
    "        self.v_max = config[\"v_max\"]\n",
    "        self.atom_size = config[\"atom_size\"]\n",
    "        self.support = np.linspace(self.v_min, self.v_max, self.atom_size)\n",
    "\n",
    "        self.transition = list()\n",
    "        self.is_test = True\n",
    "        # self.search = search.Search(\n",
    "        #     scoring_function=self.score_state,\n",
    "        #     max_depth=config[\"search_max_depth\"],\n",
    "        #     max_time=config[\"search_max_time\"],\n",
    "        #     transposition_table=search.TranspositionTable(\n",
    "        #         buckets=config[\"search_transposition_table_buckets\"],\n",
    "        #         bucket_size=config[\"search_transposition_table_bucket_size\"],\n",
    "        #         replacement_strategy=search.TranspositionTable.replacement_strategies[\n",
    "        #             config[\"search_transposition_table_replacement_strategy\"]\n",
    "        #         ],\n",
    "        #     ),\n",
    "        #     debug=False,\n",
    "        # )\n",
    "\n",
    "    def export(self, episode=-1, best_model=False):\n",
    "        if episode != -1:\n",
    "            path = \"./{}_{}_episodes.keras\".format(\n",
    "                self.model_name, episode + self.start_episode\n",
    "            )\n",
    "        else:\n",
    "            path = \"./{}.keras\".format(self.model_name)\n",
    "\n",
    "        if best_model:\n",
    "            path = \"./best_model.keras\"\n",
    "\n",
    "        self.model.save(path)\n",
    "\n",
    "    def prepare_states(self, state):\n",
    "        if (self.env.observation_space.high == 255).all():\n",
    "            state = np.array(state)/255\n",
    "        # print(state.shape)\n",
    "        if state.shape == self.observation_dimensions:\n",
    "            new_shape = (1,) + state.shape\n",
    "            state_input = state.reshape(new_shape)\n",
    "        else:\n",
    "            state_input = state\n",
    "        # print(state_input.shape)\n",
    "        # observation_high = self.env.observation_space.high\n",
    "        # observation_low = self.env.observation_space.low\n",
    "        # for s in state_input:\n",
    "        #     for i in range(len(s)):\n",
    "        #         s[i] = s[i] - observation_low[i]\n",
    "        #         s[i] = s[i] / (observation_high[i] - observation_low[i])\n",
    "        # print(state_input)\n",
    "        # NORMALIZE VALUES\n",
    "        return state_input\n",
    "\n",
    "    def predict_single(self, state):\n",
    "        state_input = self.prepare_states(state)\n",
    "        # print(state_input)\n",
    "        q_values = self.model(inputs=state_input).numpy()\n",
    "        return q_values\n",
    "\n",
    "    def select_action(self, state):\n",
    "        q_values = np.sum(np.multiply(self.predict_single(state), np.array(self.support)), axis=2)\n",
    "        # print(q_values)\n",
    "        selected_action = np.argmax(q_values)\n",
    "        # selected_action = np.argmax(self.predict_single(state))\n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "        return selected_action\n",
    "    \n",
    "    def step(self, action):\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            if self.use_n_step:\n",
    "                one_step_transition = self.memory_n.store(*self.transition)\n",
    "            else:\n",
    "                one_step_transition = self.transition\n",
    "            \n",
    "            if one_step_transition:\n",
    "                self.memory.store(*one_step_transition)\n",
    "        \n",
    "        return next_state, reward, terminated, truncated\n",
    "\n",
    "    def experience_replay(self):\n",
    "        print(\"Experience Replay\")\n",
    "        time1 = 0\n",
    "        time1 = time()\n",
    "        with tf.GradientTape() as tape:\n",
    "            print(\"One Step Learning\")\n",
    "            time2 = 0\n",
    "            # time2 = time()\n",
    "            samples = self.memory.sample(self.per_beta)\n",
    "            # actions = samples[\"actions\"]\n",
    "            # observations = samples[\"observations\"]\n",
    "            # inputs = self.prepare_states(observations)\n",
    "            weights = samples[\"weights\"].reshape(-1, 1)\n",
    "            indices = samples[\"indices\"]\n",
    "            # discount_factor = self.discount_factor\n",
    "            # target_ditributions = self.compute_target_distributions(samples, discount_factor)\n",
    "            # self.model.loss.actions = samples[\"actions\"]\n",
    "            # initial_distributions = self.model(inputs)\n",
    "            # distributions_to_train = tf.gather_nd(initial_distributions, list(zip(range(initial_distributions.shape[0]), actions)))\n",
    "            # elementwise_loss = self.model.loss.call(y_pred=distributions_to_train, y_true=tf.convert_to_tensor(target_ditributions))\n",
    "            # print(\"One Step Learning Time \", time() - time2)\n",
    "\n",
    "            if self.use_n_step:\n",
    "                print(\"N-Step Learning\")\n",
    "                time2 = time()\n",
    "                discount_factor = self.discount_factor ** self.n_step\n",
    "                n_step_samples = self.memory_n.sample_from_indices(indices)\n",
    "                actions = n_step_samples[\"actions\"]\n",
    "                observations = n_step_samples[\"observations\"]\n",
    "                inputs = self.prepare_states(observations)\n",
    "                target_ditributions = self.compute_target_distributions(n_step_samples, discount_factor)\n",
    "                self.model.loss.actions = n_step_samples[\"actions\"]\n",
    "                initial_distributions = self.model(inputs)\n",
    "                distributions_to_train = tf.gather_nd(initial_distributions, list(zip(range(initial_distributions.shape[0]), actions)))\n",
    "                elementwise_loss_n_step = self.model.loss.call(y_pred=distributions_to_train, y_true=tf.convert_to_tensor(target_ditributions))\n",
    "                # add the losses together to reduce variance (original paper just uses n_step loss)\n",
    "                elementwise_loss = elementwise_loss_n_step\n",
    "                print(\"Elementwise Loss N-Step Shape\", elementwise_loss_n_step.shape)\n",
    "                print(\"N-Step Learning Time \", time() - time2)\n",
    "\n",
    "            loss = tf.reduce_mean(elementwise_loss * weights)\n",
    "        \n",
    "        #TRAINING WITH GRADIENT TAPE\n",
    "        print(\"Computing Gradients\")\n",
    "        time2 = time()\n",
    "        gradients = tape.gradient(loss, self.model.trainable_variables)\n",
    "        print(\"Computing Gradients Time \", time() - time2)\n",
    "        print(\"Applying Gradients\")\n",
    "        time2 = time()\n",
    "        self.optimizer(learning_rate=self.learning_rate, epsilon=self.adam_epsilon, clipnorm=self.clipnorm).apply_gradients(grads_and_vars=zip(gradients, self.model.trainable_variables))\n",
    "        print(\"Applying Gradients Time \", time() - time2)\n",
    "\n",
    "        # TRAINING WITH tf.train_on_batch\n",
    "        # print(\"Training Model on Batch\")\n",
    "        # loss = self.model.train_on_batch(samples[\"observations\"], target_ditributions, sample_weight=weights)\n",
    "\n",
    "        print(\"Updating Priorities\")\n",
    "        time2 = time()\n",
    "        prioritized_loss = elementwise_loss + self.per_epsilon\n",
    "        self.memory.update_priorities(indices, prioritized_loss)\n",
    "        print(\"Updating Priorities Time \", time() - time2)\n",
    "\n",
    "        print(\"Resetting Noise\")\n",
    "        time2 = time()\n",
    "        self.model.reset_noise()\n",
    "        self.target_model.reset_noise()\n",
    "        print(\"Resetting Noise Time \", time() - time2)\n",
    "\n",
    "        loss = loss.numpy()\n",
    "        print(\"Experience Replay Time \", time() - time1)\n",
    "        return loss\n",
    "\n",
    "    def compute_target_distributions(self, samples, discount_factor):\n",
    "        print(\"Computing Target Distributions\")\n",
    "        time1 = 0\n",
    "        time1 = time()\n",
    "        observations = samples[\"observations\"]\n",
    "        inputs = self.prepare_states(observations)\n",
    "        next_observations = samples[\"next_observations\"]\n",
    "        next_inputs = self.prepare_states(next_observations)\n",
    "        rewards = samples[\"rewards\"].reshape(-1,1)\n",
    "        dones = samples[\"dones\"].reshape(-1,1)\n",
    "\n",
    "        # print(rewards.shape, dones.shape)\n",
    "\n",
    "        next_actions = np.argmax(np.sum(self.model(inputs).numpy(), axis=2), axis=1)  \n",
    "        target_network_distributions = self.target_model(next_inputs).numpy()      \n",
    "\n",
    "        target_distributions = target_network_distributions[range(self.replay_batch_size), next_actions]\n",
    "        target_z = rewards + (1 - dones) * (discount_factor) * self.support\n",
    "        # print(\"Target Z\", target_z.shape)\n",
    "        target_z = np.clip(target_z, self.v_min, self.v_max)\n",
    "\n",
    "        b = ((target_z - self.v_min) / (self.v_max - self.v_min)) * (self.atom_size - 1)\n",
    "        # print(b)\n",
    "        l, u = tf.cast(tf.math.floor(b), tf.int32), tf.cast(tf.math.ceil(b), tf.int32)\n",
    "        # print(l, u)\n",
    "        m = np.zeros_like(target_distributions)\n",
    "        assert m.shape == l.shape\n",
    "        lower_distributions = target_distributions * (tf.cast(u, tf.float64) - b)\n",
    "        upper_distributions = target_distributions * (b - tf.cast(l, tf.float64))\n",
    "\n",
    "        for i in range(self.replay_batch_size):\n",
    "            np.add.at(m[i], np.asarray(l)[i], lower_distributions[i])\n",
    "            np.add.at(m[i], np.asarray(u)[i], upper_distributions[i])\n",
    "            # print(m[i])\n",
    "        # prevent NaN\n",
    "        target_distributions = np.clip(m, 1e-3, 1)\n",
    "        print(\"Computing Target Distributions Time \", time() - time1)\n",
    "        return target_distributions\n",
    "    \n",
    "    # def score_state(self, state, turn):\n",
    "    #     state_input = self.prepare_state(state)\n",
    "    #     q = self.predict(state_input)\n",
    "\n",
    "    #     if (turn % 2) == 0:\n",
    "    #         return q.max(), q.argmax()\n",
    "\n",
    "    #     return q.min(), q.argmin()\n",
    "\n",
    "    # def play_optimal_move(\n",
    "    #     self, state: bb.Bitboard, turn: int, max_depth: int, with_output=True\n",
    "    # ):\n",
    "    #     # q_value, action = self.alpha_beta_pruning(state, turn, max_depth=max_depth)\n",
    "    #     q_value, action = self.search.iterative_deepening(state, turn, max_depth)\n",
    "    #     if with_output:\n",
    "    #         print(\"Evaluation: {}\".format(q_value))\n",
    "    #         print(\"Action: {}\".format(action + 1))\n",
    "    #     state.move(turn % 2, action)\n",
    "    #     winner, _ = state.check_victory()\n",
    "\n",
    "    #     if winner == 0:\n",
    "    #         return False\n",
    "    #     else:\n",
    "    #         return True\n",
    "\n",
    "    def action_mask(self, q, state, turn):\n",
    "        q_copy = copy.deepcopy(q)\n",
    "        for i in range(len(q_copy)):\n",
    "            if not state.is_valid_move(i):\n",
    "                if turn % 2 == 0:\n",
    "                    q_copy[i] = float(\"-inf\")\n",
    "                else:\n",
    "                    q_copy[i] = float(\"inf\")\n",
    "        return q_copy\n",
    "\n",
    "    def fill_memory(self):\n",
    "        state, _ = self.env.reset()\n",
    "        # print(state)\n",
    "        for experience in range(self.min_memory_size):\n",
    "            clear_output(wait=True)\n",
    "            print(\"Filling Memory\")\n",
    "            print(\"Memory Size: {}/{}\".format(experience, self.min_memory_size))\n",
    "            # state_input = self.prepare_state(state)\n",
    "            action = self.env.action_space.sample()\n",
    "            self.transition = [state, action]\n",
    "\n",
    "            next_state, reward, terminated, truncated = self.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "\n",
    "    def update_target_model(self, step):\n",
    "        print(\"Updating Target Model\")\n",
    "        time1 = 0\n",
    "        time1 = time()\n",
    "        if self.soft_update:\n",
    "            new_weights = self.target_model.get_weights()\n",
    "\n",
    "            counter = 0\n",
    "            for wt, wp in zip(\n",
    "                self.target_model.get_weights(),\n",
    "                self.model.get_weights(),\n",
    "            ):\n",
    "                wt = (self.ema_beta * wt) + ((1 - self.ema_beta) * wp)\n",
    "                new_weights[counter] = wt\n",
    "                counter += 1\n",
    "            self.target_model.set_weights(new_weights)\n",
    "        else:\n",
    "            if step % self.transfer_frequency == 0 and (len(self.memory) >= self.replay_batch_size):\n",
    "                self.target_model.set_weights(self.model.get_weights())\n",
    "        print(\"Updating Target Model Time \", time() - time1)\n",
    "\n",
    "    def train(self, graph_interval=200):\n",
    "        self.is_test = False\n",
    "        stat_score = []\n",
    "        stat_loss = []\n",
    "        self.fill_memory()\n",
    "        num_trials_truncated = 0\n",
    "        state, _ = self.env.reset()\n",
    "        model_update_count = 0\n",
    "        score = 0\n",
    "        for step in range(self.num_training_steps):\n",
    "            # state_input = self.prepare_state(state)\n",
    "            clear_output(wait=True)\n",
    "            print(\"Step: {}/{}\".format(step, self.num_training_steps))\n",
    "            print(\"Last Training Score: \", stat_score[-1] if len(stat_score) > 0 else 0)\n",
    "            print(\"Last Training Loss: \", stat_loss[-1] if len(stat_loss) > 0 else 0)\n",
    "            action = self.select_action(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated = self.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if truncated:\n",
    "                num_trials_truncated += 1\n",
    "            #     if num_trials_truncated > 100:\n",
    "            #         num_trials_truncated += self.num_training_steps - step\n",
    "            #         break\n",
    "            self.per_beta = min(1.0, self.per_beta + self.per_beta_increase)\n",
    "\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "                stat_score.append(score)\n",
    "                score = 0\n",
    "\n",
    "            if (step % self.replay_period) == 0 and (len(self.memory) >= self.replay_batch_size):\n",
    "                model_update_count += 1\n",
    "                loss = self.experience_replay()\n",
    "                stat_loss.append(loss)\n",
    "\n",
    "                self.update_target_model(model_update_count)\n",
    "\n",
    "            \n",
    "            if step % graph_interval == 0 and step > 0:\n",
    "                self.export()\n",
    "                self.plot_graph(stat_score, stat_loss, step)\n",
    "        self.plot_graph(stat_score, stat_loss, self.num_training_steps)\n",
    "        self.export()\n",
    "        self.env.close()\n",
    "        return num_trials_truncated / self.num_training_steps\n",
    "    \n",
    "    def plot_graph(self, score, loss, step):\n",
    "            fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "            ax1.plot(score, linestyle=\"solid\")\n",
    "            ax1.set_title('Frame {}. Score: {}'.format(step, np.mean(score[-10:])))\n",
    "            ax2.plot(loss, linestyle=\"solid\")\n",
    "            ax2.set_title('Frame {}. Loss: {}'.format(step, np.mean(loss[-10:])))\n",
    "\n",
    "            plt.savefig(\"./{}.png\".format(self.model_name))\n",
    "            plt.close(fig)\n",
    "\n",
    "    def test(self, video_folder = '') -> None:\n",
    "        \"\"\"Test the agent.\"\"\"\n",
    "        self.is_test = True\n",
    "        if video_folder == '':\n",
    "            video_folder = \"./videos/{}\".format(self.model_name)\n",
    "        # for recording a video\n",
    "        naive_env = self.env\n",
    "        self.env = gym.wrappers.RecordVideo(self.env, video_folder)\n",
    "        state, _ = self.env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        \n",
    "        while not done:            \n",
    "            action = self.select_action(state)\n",
    "            # self.env.render()\n",
    "            next_state, reward, terminated, truncated = self.step(action)\n",
    "            done = terminated or truncated\n",
    "            state = next_state\n",
    "\n",
    "            score += reward\n",
    "        \n",
    "        print(\"score: \", score)\n",
    "        self.env.close()\n",
    "        \n",
    "        # reset\n",
    "        self.env = naive_env\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeZeroToOne(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_high = self.env.observation_space.high\n",
    "        self.observation_low = self.env.observation_space.low\n",
    "\n",
    "    def observation(self, obs):\n",
    "        print(obs)\n",
    "        print((obs - self.observation_low) / (self.observation_high - self.observation_low))\n",
    "        return (obs - self.observation_low) / (self.observation_high - self.observation_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online with frame stackign though\n",
    "# env = gym.wrappers.AtariPreprocessing(gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\"), terminal_on_life_loss=True, scale_obs=True) # as seen online\n",
    "env = ClipReward(gym.wrappers.AtariPreprocessing(gym.make(\"MsPacmanNoFrameskip-v4\", render_mode=\"rgb_array\"), terminal_on_life_loss=True), -1, 1) # as recommended by the original paper, should already include max pooling\n",
    "# env = gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\")\n",
    "env = gym.wrappers.FrameStack(env, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 133/3000\n",
      "Last Training Score:  11.0\n",
      "Last Training Loss:  2.017629\n",
      "Storing in Buffer\n",
      "Buffer Storage Time  -1.4066696166992188e-05\n",
      "Storing in PrioritizedReplayBuffer\n",
      "Storing in Buffer\n",
      "Buffer Storage Time  -5.9604644775390625e-06\n",
      "Storing in PrioritizedReplayBuffer Time  0.0132598876953125\n",
      "Experience Replay\n",
      "One Step Learning\n",
      "Sampling from PrioritizedReplayBuffer\n",
      "Getting Indices from PrioritizedReplayBuffer Sum Tree\n",
      "Getting Indices from PrioritizedReplayBuffer Sum Tree Time  1.280674934387207\n",
      "Retrieving Data from PrioritizedReplayBuffer Data Arrays\n",
      "Retrieving Data from PrioritizedReplayBuffer Data Arrays Time  0.10491299629211426\n",
      "Sampling from PrioritizedReplayBuffer Time  1.3858692646026611\n",
      "N-Step Learning\n",
      "Sampling From Indices\n",
      "Computing Target Distributions\n",
      "Computing Target Distributions Time  0.05027890205383301\n",
      "Elementwise Loss N-Step Shape (128,)\n",
      "N-Step Learning Time  0.059967994689941406\n",
      "Computing Gradients\n",
      "Computing Gradients Time  0.010220050811767578\n",
      "Applying Gradients\n",
      "Applying Gradients Time  0.06075692176818848\n",
      "Updating Priorities\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 75\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m     74\u001b[0m agent \u001b[38;5;241m=\u001b[39m RainbowDQN(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRainbowDQN-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env\u001b[38;5;241m.\u001b[39munwrapped\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mid), config\u001b[38;5;241m=\u001b[39mconfig)\n\u001b[0;32m---> 75\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 382\u001b[0m, in \u001b[0;36mRainbowDQN.train\u001b[0;34m(self, graph_interval)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_period) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_batch_size):\n\u001b[1;32m    381\u001b[0m     model_update_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 382\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience_replay\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     stat_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_target_model(model_update_count)\n",
      "Cell \u001b[0;32mIn[36], line 221\u001b[0m, in \u001b[0;36mRainbowDQN.experience_replay\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m time2 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m    220\u001b[0m prioritized_loss \u001b[38;5;241m=\u001b[39m elementwise_loss \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mper_epsilon\n\u001b[0;32m--> 221\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_priorities\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprioritized_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdating Priorities Time \u001b[39m\u001b[38;5;124m\"\u001b[39m, time() \u001b[38;5;241m-\u001b[39m time2)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResetting Noise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 82\u001b[0m, in \u001b[0;36mPrioritizedReplayBuffer.update_priorities\u001b[0;34m(self, indices, priorities)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msum_tree[index] \u001b[38;5;241m=\u001b[39m priority \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_tree\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m priority \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_priority \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_priority, priority)\n",
      "Cell \u001b[0;32mIn[16], line 70\u001b[0m, in \u001b[0;36mSegmentTree.__setitem__\u001b[0;34m(self, idx, val)\u001b[0m\n\u001b[1;32m     68\u001b[0m idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree[idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moperation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     idx \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/ops.py:300\u001b[0m, in \u001b[0;36m_EagerTensorBase.__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 300\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/tensorflow/python/framework/ops.py:360\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# config = {\n",
    "#     'conv_layers': [(32, 8, (4, 4)), (64, 4, (2, 2)), (64, 3, (1, 1))],\n",
    "#     'conv_layers_noisy': False,\n",
    "#     'dense_layers': [512],\n",
    "#     'dense_layers_noisy': True,\n",
    "#     'noisy_sigma': 0.5,\n",
    "#     'activation': 'relu',\n",
    "#     'kernel_initializer': initializers.VarianceScaling(scale=1.0/np.sqrt(3.0), mode='fan_in', distribution='uniform'),\n",
    "#     'optimizer_function': tf.keras.optimizers.legacy.Adam,\n",
    "#     'adam_epsilon': 1.5e-4,\n",
    "#     'learning_rate': 0.0000625,\n",
    "#     'loss_function': tf.keras.losses.CategoricalCrossentropy(), #? KL Divergence? \n",
    "#     'dueling': True,\n",
    "#     'advantage_hidden_layers': [],\n",
    "#     'value_hidden_layers': [],\n",
    "#     'num_training_steps': 1000000, #\n",
    "#     'discount_factor': 0.99,\n",
    "#     'soft_update': False,\n",
    "#     'ema_beta': 0.99,\n",
    "#     'transfer_frequency': 32000,\n",
    "#     'per_epsilon': 1e-6, #\n",
    "#     'per_alpha': 0.5,\n",
    "#     'per_beta': 0.4,\n",
    "#     'per_beta_increase': 1/1000000,\n",
    "#     'replay_batch_size': 32,\n",
    "#     'replay_period': 4,\n",
    "#     'memory_size': 1000000,\n",
    "#     'min_memory_size': 80000,\n",
    "#     'n_step': 3,\n",
    "#     'v_min': -10.0,\n",
    "#     'v_max': 10.0,\n",
    "#     'atom_size': 51,\n",
    "#     'search_max_depth': 5, \n",
    "#     'search_max_time': 10,\n",
    "# }\n",
    "\n",
    "config = {\n",
    "    'conv_layers': [],\n",
    "    'conv_layers_noisy': False,\n",
    "    'dense_layers': [128],\n",
    "    'dense_layers_noisy': False,\n",
    "    'noisy_sigma': 0.5,\n",
    "    'activation': 'relu',\n",
    "    'kernel_initializer': initializers.VarianceScaling(scale=1.0/np.sqrt(3.0), mode='fan_in', distribution='uniform'),\n",
    "    'optimizer_function': tf.keras.optimizers.legacy.Adam,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'learning_rate': 1e-3,\n",
    "    'loss_function': tf.keras.losses.KLDivergence(), #? KL Divergence? \n",
    "    'dueling': True,\n",
    "    'advantage_hidden_layers': [128],\n",
    "    'value_hidden_layers': [128],\n",
    "    'num_training_steps': 3000, #\n",
    "    'discount_factor': 0.99,\n",
    "    'soft_update': False,\n",
    "    'ema_beta': 0.99,\n",
    "    'transfer_frequency': 100,\n",
    "    'per_epsilon': 1e-6, #\n",
    "    'per_alpha': 0.2,\n",
    "    'per_beta': 0.6,\n",
    "    'per_beta_increase': 1/10000,\n",
    "    'replay_batch_size': 128,\n",
    "    'replay_period': 1,\n",
    "    'memory_size': 10000,\n",
    "    'min_memory_size': 0,\n",
    "    'n_step': 3,\n",
    "    'v_min': 0.0,\n",
    "    'v_max': 500.0,\n",
    "    'atom_size': 51,\n",
    "    'search_max_depth': 5, \n",
    "    'search_max_time': 10,\n",
    "}\n",
    "\n",
    "# train\n",
    "agent = RainbowDQN(env, \"RainbowDQN-{}\".format(env.unwrapped.spec.id), config=config)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import glob\n",
    "import io\n",
    "import os\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "\n",
    "def ipython_show_video(path: str) -> None:\n",
    "    \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
    "    if not os.path.isfile(path):\n",
    "        raise NameError(\"Cannot access: {}\".format(path))\n",
    "\n",
    "    video = io.open(path, \"r+b\").read()\n",
    "    encoded = base64.b64encode(video)\n",
    "\n",
    "    display(HTML(\n",
    "        data=\"\"\"\n",
    "        <video width=\"320\" height=\"240\" alt=\"test\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
    "        </video>\n",
    "        \"\"\".format(encoded.decode(\"ascii\"))\n",
    "    ))\n",
    "\n",
    "\n",
    "def show_latest_video(video_folder: str) -> str:\n",
    "    \"\"\"Show the most recently recorded video from video folder.\"\"\"\n",
    "    list_of_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
    "    latest_file = max(list_of_files, key=os.path.getctime)\n",
    "    ipython_show_video(latest_file)\n",
    "    return latest_file\n",
    "\n",
    "\n",
    "latest_file = show_latest_video(video_folder='./video')\n",
    "print(\"Played:\", latest_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "import tensorflow as tf\n",
    "from hyperopt.pyll import scope\n",
    "\n",
    "def create_search_space():\n",
    "    search_space = {\n",
    "        'conv_layers': hp.choice('conv_layers', [[]]),\n",
    "        'conv_layers_noisy': hp.choice('conv_layers_noisy', [False]), #\n",
    "        'dense_layers': hp.choice('dense_layers', [\n",
    "            [], [32], [32, 32], [32, 32, 32], [64], [64, 64], [64, 64, 64], [128], [128, 128], [128, 128, 128], [256], [256, 256], [256, 256, 256], [512], [512, 512], [512, 512, 512], [1024], [1024, 1024], [1024, 1024, 1024] \n",
    "        ]),\n",
    "        'dense_layers_noisy': hp.choice('dense_layers_noisy', [True, False]), #\n",
    "        'noisy_sigma': hp.uniform('noisy_sigma', 0.1, 1.0), #\n",
    "        'activation': hp.choice('activation', ['relu', 'sigmoid']),\n",
    "        'kernel_initializer': hp.choice('kernel_initializer', [tf.keras.initializers.GlorotNormal(), tf.keras.initializers.Uniform(), tf.keras.initializers.HeNormal(), tf.keras.initializers.HeUniform(), tf.keras.initializers.VarianceScaling(scale=1.0/np.sqrt(3.0), mode='fan_in', distribution='uniform'), tf.keras.initializers.VarianceScaling(scale=2.0, mode='fan_in', distribution='uniform')]),\n",
    "        'optimizer_function': hp.choice('optimizer_function', [tf.keras.optimizers.legacy.Adam, tf.keras.optimizers.legacy.SGD]),\n",
    "        'adam_epsilon': hp.uniform('adam_epsilon', 0.00001, 1.0), # \n",
    "        'learning_rate': hp.uniform('learning_rate', 0.000001, 0.0025),\n",
    "        'loss_function': hp.choice('loss_function', [tf.keras.losses.CategoricalCrossentropy(), tf.keras.losses.KLDivergence()]),\n",
    "        'dueling': hp.choice('dueling', [True]),\n",
    "        'advantage_hidden_layers': [], #\n",
    "        'value_hidden_layers': [], #\n",
    "        'num_training_steps': scope.int(hp.quniform('num_training_steps', 5000, 10000, 100)), #\n",
    "        'discount_factor': hp.uniform('discount_factor', 0.85, 0.999),\n",
    "        'soft_update': hp.choice('soft_update', [True, False]),\n",
    "        'ema_beta': hp.uniform('ema_beta', 0.95, 0.999),\n",
    "        'transfer_frequency': scope.int(hp.uniform('transfer_frequency', 0, 200)), #\n",
    "        'per_epsilon': hp.uniform('per_epsilon', 0.000001, 0.1),\n",
    "        'per_alpha': hp.choice('per_alpha', [0.05 * i for i in range(0, 21)]),\n",
    "        'per_beta': hp.choice('per_beta', [0.05 * i for i in range(0, 21)]),\n",
    "        'per_beta_increase': hp.uniform('per_beta_increase', 0, 0.015),\n",
    "        'replay_batch_size': hp.choice('replay_batch_size', [2 ** i for i in range(0, 8)]),\n",
    "        'replay_period': scope.int(hp.quniform('replay_period', 1, 10, 1)),\n",
    "        'memory_size': scope.int(hp.quniform('memory_size', 1, 100000, 1)), #\n",
    "        'min_memory_size': hp.uniform('min_memory_size', 0, 3000), #\n",
    "        'n_step': scope.int(hp.quniform('n_step', 1, 5, 1)), #\n",
    "        'v_min': hp.choice('v_min', [0.0]), #\n",
    "        'v_max': scope.int(hp.quniform('v_max', 100.0, 1000.0, 100.0)),\n",
    "        'atom_size': hp.choice('atom_size', [51]), #\n",
    "        # 'search_max_depth': 5, \n",
    "        # 'search_max_time': 10,\n",
    "    }\n",
    "    # Current best setting\n",
    "    # For hp.uniform specify the exact value\n",
    "    # For hp.choice specify the index (0 based indexing) in the array\n",
    "    initial_best_config = []\n",
    "\n",
    "    return search_space, initial_best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def list_scores(trials):\n",
    "    print(\"{} Scores\".format(env.unwrapped.spec.id))\n",
    "    trials = pickle.load(open(\"./{}_trials.p\".format(env.unwrapped.spec.id), \"rb\"))\n",
    "\n",
    "    total_score = 0\n",
    "    scores = []\n",
    "    for trial in trials.trials:\n",
    "        total_score += -1 * trial['result']['loss']\n",
    "        scores.append((trial['tid'] + 1, int(-1 * trial['result']['loss'])))\n",
    "\n",
    "    scores.sort(key=lambda x: x[1])\n",
    "    scores.reverse()\n",
    "    for score in scores:\n",
    "        print(\"Model {} | Score: {}\".format(score[0], score[1]))\n",
    "\n",
    "    average_score = int(total_score / len(trials.trials))\n",
    "    print(\"Average Score: \", average_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import tpe, hp, fmin\n",
    "import numpy as np\n",
    "import pandas\n",
    "import datetime\n",
    "from hyperopt.base import Trials\n",
    "from hyperopt import space_eval\n",
    "import logging\n",
    "import pickle\n",
    "import copy \n",
    "from itertools import product\n",
    "import os\n",
    "\n",
    "search_space, initial_best_config = create_search_space()\n",
    "\n",
    "def objective(params):\n",
    "    if os.path.exists(\"./{}_trials.p\".format(env.unwrapped.spec.id)):\n",
    "        trials = pickle.load(open(\"./{}_trials.p\".format(env.unwrapped.spec.id), \"rb\"))\n",
    "        name = \"{}_{}\".format(env.unwrapped.spec.id, len(trials.trials) + 1)\n",
    "    else:\n",
    "        name = \"{}_1\".format(env.unwrapped.spec.id)\n",
    "    # name = datetime.datetime.now().timestamp()\n",
    "    params[\"model_name\"] = name\n",
    "    entry = pandas.DataFrame.from_dict(\n",
    "        params,\n",
    "        orient=\"index\",\n",
    "    ).T\n",
    "\n",
    "    entry.to_csv(\n",
    "        \"{}_results.csv\".format(env.unwrapped.spec.id),\n",
    "        mode=\"a\",\n",
    "        header=False,\n",
    "    )\n",
    "\n",
    "    m = RainbowDQN(\n",
    "        env=env,\n",
    "        model_name=\"{}\".format(name),\n",
    "        config=params\n",
    "    )\n",
    "\n",
    "    num_trials_truncated = m.train(save_plots=True)\n",
    "    score = 0\n",
    "    for i in range(5):\n",
    "        score += m.test()\n",
    "    score = score / 5\n",
    "    score += num_trials_truncated * 100\n",
    "    return score\n",
    "\n",
    "\n",
    "def no_progress_loss(iteration_stop_count=20, percent_increase=0.0):\n",
    "    def stop_fn(trials, best_loss=None, iteration_no_progress=0):\n",
    "        new_loss = trials.trials[len(trials.trials)-1]['result']['loss']\n",
    "        print(new_loss)\n",
    "        if best_loss is None:\n",
    "            return False, [new_loss, iteration_no_progress+1]\n",
    "        best_loss_threshold = best_loss - abs(best_loss * (percent_increase/100.0))\n",
    "        if new_loss < best_loss_threshold:\n",
    "            best_loss = new_loss\n",
    "            iteration_no_progress = 0\n",
    "        else:\n",
    "            iteration_no_progress += 1\n",
    "\n",
    "        return iteration_no_progress >= iteration_stop_count, [best_loss, iteration_no_progress]\n",
    "\n",
    "    return stop_fn\n",
    "\n",
    "max_trials = 3\n",
    "trials_step = 10  # how many additional trials to do after loading the last ones\n",
    "\n",
    "try:  # try to load an already saved trials object, and increase the max\n",
    "    trials = pickle.load(open(\"./{}_trials.p\".format(env.unwrapped.spec.id), \"rb\"))\n",
    "    print(\"Found saved Trials! Loading...\")\n",
    "    max_trials = len(trials.trials) + trials_step\n",
    "    print(\"Rerunning from {} trials to {} (+{}) trials\".format(len(trials.trials), max_trials, trials_step))\n",
    "except:  # create a new trials object and start searching\n",
    "    # trials = Trials()\n",
    "    trials = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = fmin(\n",
    "    fn=objective, # Objective Function to optimize\n",
    "    space=search_space, # Hyperparameter's Search Space\n",
    "    algo=tpe.suggest, # Optimization algorithm (representative TPE)\n",
    "    max_evals=max_trials, # Number of optimization attempts\n",
    "    trials=trials, # Record the results\n",
    "    early_stop_fn=no_progress_loss(5, 1),\n",
    "    trials_save_file=\"./{}_trials.p\".format(env.unwrapped.spec.id),\n",
    "    points_to_evaluate=initial_best_config\n",
    ")\n",
    "\n",
    "print(best)\n",
    "best_trial = space_eval(search_space, best)\n",
    "# print(best_trial)\n",
    "# with open(\"best_params.p\", mode=\"wb\") as f:\n",
    "#     pickle.dump(best_trial, f)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
